{"cells":[{"cell_type":"markdown","id":"1df29eda","metadata":{"id":"1df29eda"},"source":["Step 0. Unzip enron1.zip into the current directory."]},{"cell_type":"markdown","id":"bf32cfce","metadata":{"id":"bf32cfce"},"source":["Step 1. Traverse the dataset and create a Pandas dataframe. This is already done for you and should run without any errors. You should recognize Pandas from task 1."]},{"cell_type":"code","execution_count":23,"id":"20c5d195","metadata":{"id":"20c5d195"},"outputs":[{"name":"stdout","output_type":"stream","text":["skipped 2248.2004-09-23.GP.spam.txt\n","skipped 2526.2004-10-17.GP.spam.txt\n","skipped 2698.2004-10-31.GP.spam.txt\n","skipped 4566.2005-05-24.GP.spam.txt\n"]}],"source":["import pandas as pd\n","import os\n","\n","def read_spam():\n","    category = 'spam'\n","    directory = './enron1/spam'\n","    return read_category(category, directory)\n","\n","def read_ham():\n","    category = 'ham'\n","    directory = './enron1/ham'\n","    return read_category(category, directory)\n","\n","def read_category(category, directory):\n","    emails = []\n","    for filename in os.listdir(directory):\n","        if not filename.endswith(\".txt\"):\n","            continue\n","        with open(os.path.join(directory, filename), 'r') as fp:\n","            try:\n","                content = fp.read()\n","                emails.append({'name': filename, 'content': content, 'category': category})\n","            except:\n","                print(f'skipped {filename}')\n","    return emails\n","\n","ham = read_ham()\n","spam = read_spam()\n","\n","df1 = pd.DataFrame.from_records(ham)\n","df2 = pd.DataFrame.from_records(spam)\n","df = pd.concat([df1, df2], ignore_index=True)"]},{"cell_type":"markdown","id":"1a1c23fd","metadata":{"id":"1a1c23fd"},"source":["Step 2. Data cleaning is a critical part of machine learning. You and I can recognize that 'Hello' and 'hello' are the same word but a machine does not know this a priori. Therefore, we can 'help' the machine by conducting such normalization steps for it. Write a function `preprocessor` that takes in a string and replaces all non alphabet characters with a space and then lowercases the result."]},{"cell_type":"code","execution_count":24,"id":"c447c901","metadata":{"id":"c447c901"},"outputs":[],"source":["import re\n","\n","def preprocessor(e):\n","    return re.sub('[^A-Za-z]', ' ', e).lower()"]},{"cell_type":"markdown","id":"ba32521d","metadata":{"id":"ba32521d"},"source":["Step 3. We will now train the machine learning model. All the functions that you will need are imported for you. The instructions explain how the work and hint at which functions to use. You will likely need to refer to the scikit learn documentation to see how exactly to invoke the functions. It will be handy to keep that tab open."]},{"cell_type":"code","execution_count":25,"id":"1442d377","metadata":{"id":"1442d377"},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix: \n","[[717  12]\n"," [ 14 291]]\n","\n","Detailed Statistics: \n","              precision    recall  f1-score   support\n","\n","         ham       0.98      0.98      0.98       729\n","        spam       0.96      0.95      0.96       305\n","\n","    accuracy                           0.97      1034\n","   macro avg       0.97      0.97      0.97      1034\n","weighted avg       0.97      0.97      0.97      1034\n","\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","vectorizer = CountVectorizer(preprocessor=preprocessor)\n","X_train, X_test, y_train, y_test = train_test_split(df['content'],df['category'], test_size=0.2, random_state=42)\n","X_train_df = vectorizer.fit_transform(X_train)\n","\n","model = LogisticRegression()\n","model.fit(X_train_df, y_train)\n","\n","X_test_df = vectorizer.transform(X_test)\n","y_pred = model.predict(X_test_df)\n","\n","print(f'Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}\\n')\n","print(f'Detailed Statistics: \\n{classification_report(y_test, y_pred)}')\n"]},{"cell_type":"markdown","id":"9674d032","metadata":{"id":"9674d032"},"source":["Step 4."]},{"cell_type":"code","execution_count":26,"id":"6b7d78c9","metadata":{"id":"6b7d78c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 10 Positive Features\n","   Feature Index  Importance   Feature\n","0          24309    0.920089        no\n","1          17180    0.853581      http\n","2          27576    0.851243    prices\n","3          29417    0.760439    remove\n","4          16446    0.736283     hello\n","5          25120    0.711263      only\n","6          29418    0.677409   removed\n","7          16506    0.663679      here\n","8          23302    0.625946      more\n","9          25757    0.622317  paliourg\n","\n","Top 10 Negative Features\n","   Feature Index  Importance   Feature\n","0          12156   -1.493970     enron\n","1          34478   -1.459408    thanks\n","2           2474   -1.368536  attached\n","3          10653   -1.325974       doc\n","4           9141   -1.295490     daren\n","5          26651   -1.292209  pictures\n","6          38503   -1.227058       xls\n","7           9328   -1.154972      deal\n","8          24004   -1.152822      neon\n","9          17096   -1.043461       hpl\n"]}],"source":["features = vectorizer.get_feature_names_out()\n","importance = model.coef_[0]\n","\n","indexed_importance = list(enumerate(importance))\n","sorted_importance = sorted(indexed_importance, key=lambda x: abs(x[1]), reverse=True)\n","\n","top_positive = [(index, importance) for index, importance in sorted_importance if importance > 0][:10]\n","top_negative = [(index, importance) for index, importance in sorted_importance if importance < 0][:10]\n","\n","df_top_positive = pd.DataFrame(top_positive, columns=['Feature Index', 'Importance'])\n","df_top_negative = pd.DataFrame(top_negative, columns=['Feature Index', 'Importance'])\n","\n","df_top_positive['Feature'] = [features[index] for index, _ in top_positive]\n","df_top_negative['Feature'] = [features[index] for index, _ in top_negative]\n","\n","print('Top 10 Positive Features')\n","print(df_top_positive)\n","\n","print('\\nTop 10 Negative Features')\n","print(df_top_negative)"]},{"cell_type":"markdown","id":"LI4u_ZUGToDQ","metadata":{"id":"LI4u_ZUGToDQ"},"source":["All Done!"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"task3.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":5}
